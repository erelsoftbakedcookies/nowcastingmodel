{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3.7\n",
    "\"\"\"\n",
    "Built for Hackathon to predict economic impact of COVID-19 to ADB country members\n",
    "\n",
    "Usage:\n",
    "    python3.7 -m notebook\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data\n",
    "db_file = './COVID19_Challenge_Database_17Apr_2020.xlsx'\n",
    "df = pd.read_excel(db_file, sheet_name = [0,1,2,3], header = [0,1])\n",
    "detail = df[0]\n",
    "a = df[1]\n",
    "q = df[2]\n",
    "m = df[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data cleaning a\n",
    "a.drop(a.columns[[27]],axis=1,inplace=True)\n",
    "headera = list(np.arange(2000,2021))\n",
    "a.columns = [\"Member\",\"Indicator\",\"Unit\",\n",
    "           \"Classification\",\"Subclassification\",\"Remarks\"] + headera\n",
    "nrow,ncol = a.shape\n",
    "a_data_length = 21\n",
    "a.loc[a['Unit'].isnull(),'Unit'] = ''\n",
    "a[a[headera].applymap(np.isreal)==False] = np.nan\n",
    "a[headera] = a[headera].astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data cleaning detail\n",
    "detail.columns=['countrynames','adbmember','cluster',\n",
    "                'nationalaccountsm','nationalaccountsq','nationalaccountsa',\n",
    "                'prodnretailsalesm','prodnretailsalesq','prodnretailsalesa',\n",
    "                'tradem','tradeq','tradea',\n",
    "                'remittancesm','remittancesq','remittancea',\n",
    "                'tourismarrivalsnreceiptsm','tourismarrivalsnreceiptsq','tourismarrivalsnreceiptsa',\n",
    "                'inflationm','inflationq','inflationa',\n",
    "                'fxm','fxq','fxa']\n",
    "detail=detail[1:][:-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data cleaning q\n",
    "q = q[1:]\n",
    "headerq = []\n",
    "for y in list(range(2000,2021)):\n",
    "    for quar in [\"-Q1\",\"-Q2\",\"-Q3\",\"-Q4\"]:\n",
    "        headerq.append(str(y) + quar)\n",
    "q.columns = [\"Member\",\"Indicator\",\"Unit\",\n",
    "           \"Classification\",\"Subclassification\",\"Remarks\"] + headerq\n",
    "q_data_length = 21*4\n",
    "q.loc[q['Unit'].isnull(),'Unit'] = ''\n",
    "q[q[headerq].applymap(np.isreal)==False] = np.nan\n",
    "q[headerq] = q[headerq].astype(np.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data cleaning m\n",
    "m = m[1:]\n",
    "headerm = []\n",
    "for y in list(range(2000,2021)):\n",
    "    for mon in [\"-Jan\",\"-Feb\",\"-Mar\",\"-Apr\",\"-May\",\"-Jun\",\"-Jul\",\"-Aug\",\"-Sep\",\"-Oct\",\"-Nov\",\"-Dec\"]:\n",
    "        headerm.append(str(y) + mon)\n",
    "m.columns=[\"Member\",\"Indicator\",\"Unit\",\n",
    "           \"Classification\",\"Subclassification\",\"Remarks\"] + headerm\n",
    "m_data_length = 21*12\n",
    "m.loc[m['Unit'].isnull(),'Unit'] = ''\n",
    "m[m[headerm].applymap(np.isreal)==False] = np.nan\n",
    "m[headerm] = m[headerm].astype(np.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Stiching\n",
    "\n",
    "### China, its currency is in LCU/100US$\n",
    "q.loc[(q['Member'].str.contains('Republic of China'))&(q['Classification']=='Exchange Rate'),'2000-Q1':'2020-Q4'] = q.loc[(q['Member'].str.contains('Republic of China'))&(q['Classification']=='Exchange Rate'),'2000-Q1':'2020-Q4'] / 100.0\n",
    "\n",
    "### Viet Nam\n",
    "tofix = q.loc[(q['Member']=='Viet Nam')&(q['Classification']=='National accounts'),'2000-Q1':'2020-Q4']\n",
    "tofix.loc[:,(np.arange(tofix.shape[1])+1)%4==0] = tofix.loc[:,(np.arange(tofix.shape[1])+1)%4==0].values - tofix.loc[:,(np.arange(tofix.shape[1])+1)%4==3].values\n",
    "tofix.loc[:,(np.arange(tofix.shape[1])+1)%4==3] = tofix.loc[:,(np.arange(tofix.shape[1])+1)%4==3].values - tofix.loc[:,(np.arange(tofix.shape[1])+1)%4==2].values\n",
    "tofix.loc[:,(np.arange(tofix.shape[1])+1)%4==2] = tofix.loc[:,(np.arange(tofix.shape[1])+1)%4==2].values - tofix.loc[:,(np.arange(tofix.shape[1])+1)%4==1].values\n",
    "q.loc[(q['Member']=='Viet Nam')&(q['Classification']=='National accounts'),'2000-Q1':'2020-Q4'] = tofix.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gather information\n",
    "country_names = np.unique(np.array(detail['adbmember']))\n",
    "\n",
    "### Generate FX: LCU/USD\n",
    "fx = {}\n",
    "for country_name in country_names:\n",
    "    fx[country_name] = {}\n",
    "    temp=a.loc[(a['Member']==country_name) & (a['Subclassification']=='Average Exchange Rate')]\n",
    "    if temp.shape[0]>0:\n",
    "        fx[country_name]['a']=temp.iloc[0,6:(6+a_data_length)].values.astype(np.float)\n",
    "    else:\n",
    "        fx[country_name]['a']=np.zeros(shape=(1,a_data_length))\n",
    "    temp=q.loc[(q['Member']==country_name) & (q['Subclassification']=='Average Exchange Rate')]\n",
    "    if temp.shape[0]>0:\n",
    "        fx[country_name]['q']=temp.iloc[0,6:(6+q_data_length)].values.astype(np.float)\n",
    "    else:\n",
    "        fx[country_name]['q']=np.zeros(shape=(1,q_data_length))\n",
    "    temp=m.loc[(m['Member']==country_name) & (m['Subclassification']=='Average Exchange Rate')]\n",
    "    if temp.shape[0]>0:\n",
    "        fx[country_name]['m']=temp.iloc[0,6:(6+m_data_length)].values.astype(np.float)\n",
    "    else:\n",
    "        fx[country_name]['m']=np.zeros(shape=(1,m_data_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Derive information such as magnitude and currency unit used\n",
    "### We will later use it to standardize the value\n",
    "\n",
    "### Annual data\n",
    "a['Mag'] = 0\n",
    "a.loc[a['Unit'].str.contains('Thous|Thousand',regex=True),'Mag'] = 3\n",
    "a.loc[a['Unit'].str.contains('Million|Milllion|mn',regex=True),'Mag'] = 6\n",
    "a.loc[a['Unit'].str.contains('Billion|Billlion',regex=True),'Mag'] = 9\n",
    "### Custom\n",
    "a.loc[83,'Mag'] = 5\n",
    "a.loc[205,'Mag'] = 7\n",
    "a.loc[254:258,'Mag'] = 8\n",
    "a.loc[472,'Mag'] = 8\n",
    "\n",
    "a['UnitDol'] = 'LCU'\n",
    "a.loc[a['Unit'].str.contains('US|USD',regex=True),'UnitDol'] = 'USD'\n",
    "a.loc[a['Unit'].str.contains('%|LCU/US|Index|index',regex=True),'UnitDol'] = 'Ratio'\n",
    "a.loc[a['Unit'].str.contains('Persons|Trips|persons',regex=True),'UnitDol'] = 'Unit'\n",
    "\n",
    "### Quarterly data\n",
    "q['Mag'] = 0\n",
    "q.loc[q['Unit'].str.contains('Thous|Thousand',regex=True),'Mag'] = 3\n",
    "q.loc[q['Unit'].str.contains('Million|Milllion|mn|Mil.|mil.',regex=True),'Mag'] = 6\n",
    "q.loc[q['Unit'].str.contains('Billion|Billlion|Bil.|bil.',regex=True),'Mag'] = 9\n",
    "### Custom\n",
    "q.loc[57,'Mag'] = 5\n",
    "q.loc[125,'Mag'] = 7\n",
    "q.loc[154:158,'Mag'] = 8\n",
    "q.loc[270,'Mag'] = 8\n",
    "q.loc[272:274,'Mag'] = 9\n",
    "\n",
    "q['UnitDol'] = 'LCU'\n",
    "q.loc[q['Unit'].str.contains('US|USD',regex=True),'UnitDol'] = 'USD'\n",
    "q.loc[q['Unit'].str.contains('%|LCU/US|Index|index|=100',regex=True),'UnitDol'] = 'Ratio'\n",
    "q.loc[q['Unit'].str.contains('Persons|Trips|persons',regex=True),'UnitDol'] = 'Unit'\n",
    "\n",
    "### Monthly data\n",
    "m['Mag'] = 0\n",
    "m.loc[m['Unit'].str.contains('Thous|Thousand',regex=True),'Mag'] = 3\n",
    "m.loc[m['Unit'].str.contains('Million|Milllion|mn|Mil.|mil.',regex=True),'Mag'] = 6\n",
    "m.loc[m['Unit'].str.contains('Billion|Billlion|Bil.|bil.',regex=True),'Mag'] = 9\n",
    "### Custom\n",
    "m.loc[19,'Mag'] = 5\n",
    "m.loc[67,'Mag'] = 7\n",
    "m.loc[80:84,'Mag'] = 8\n",
    "m.loc[151,'Mag'] = 8\n",
    "\n",
    "m['UnitDol'] = 'LCU'\n",
    "m.loc[m['Unit'].str.contains('US|USD',regex=True),'UnitDol'] = 'USD'\n",
    "m.loc[m['Unit'].str.contains('%|LCU/US|Index|index|=100',regex=True),'UnitDol'] = 'Ratio'\n",
    "m.loc[m['Unit'].str.contains('Persons|Trips|persons',regex=True),'UnitDol'] = 'Unit'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEXT:\n",
    "# 1. Modify unit to million USD\n",
    "target_mag = 6\n",
    "au = a.copy()\n",
    "qu = q.copy()\n",
    "mu = m.copy()\n",
    "for country_name in country_names:\n",
    "    au.loc[(au['Member']==country_name)&(au['UnitDol']=='LCU'),headera] = (au.loc[(au['Member']==country_name)&(au['UnitDol']=='LCU'),headera].mul(10.0**(au.loc[(au['Member']==country_name)&(au['UnitDol']=='LCU'),'Mag']-target_mag), axis=0)) / fx[country_name]['a']\n",
    "    au.loc[(au['Member']==country_name)&(au['UnitDol']=='USD'),headera] = (au.loc[(au['Member']==country_name)&(au['UnitDol']=='USD'),headera].mul(10.0**(au.loc[(au['Member']==country_name)&(au['UnitDol']=='USD'),'Mag']-target_mag), axis=0))\n",
    "    au.loc[(au['Member']==country_name)&(au['UnitDol']=='LCU'),'Mag'] = target_mag\n",
    "    au.loc[(au['Member']==country_name)&(au['UnitDol']=='USD'),'Mag'] = target_mag\n",
    "    \n",
    "    qu.loc[(qu['Member']==country_name)&(qu['UnitDol']=='LCU'),headerq] = (qu.loc[(qu['Member']==country_name)&(qu['UnitDol']=='LCU'),headerq].mul(10.0**(qu.loc[(qu['Member']==country_name)&(qu['UnitDol']=='LCU'),'Mag']-target_mag), axis=0)) / fx[country_name]['q']\n",
    "    qu.loc[(qu['Member']==country_name)&(qu['UnitDol']=='USD'),headerq] = (qu.loc[(qu['Member']==country_name)&(qu['UnitDol']=='USD'),headerq].mul(10.0**(qu.loc[(qu['Member']==country_name)&(qu['UnitDol']=='USD'),'Mag']-target_mag), axis=0))\n",
    "    qu.loc[(qu['Member']==country_name)&(qu['UnitDol']=='LCU'),['Mag', 'UnitDol']] = [target_mag,'USD']\n",
    "    qu.loc[(qu['Member']==country_name)&(qu['UnitDol']=='USD'),'Mag'] = target_mag\n",
    "    \n",
    "    mu.loc[(mu['Member']==country_name)&(mu['UnitDol']=='LCU'),headerm] = (mu.loc[(mu['Member']==country_name)&(mu['UnitDol']=='LCU'),headerm].mul(10.0**(mu.loc[(mu['Member']==country_name)&(mu['UnitDol']=='LCU'),'Mag']-target_mag), axis=0)) / fx[country_name]['m']\n",
    "    mu.loc[(mu['Member']==country_name)&(mu['UnitDol']=='USD'),headerm] = (mu.loc[(mu['Member']==country_name)&(mu['UnitDol']=='USD'),headerm].mul(10.0**(mu.loc[(mu['Member']==country_name)&(mu['UnitDol']=='USD'),'Mag']-target_mag), axis=0))\n",
    "    mu.loc[(mu['Member']==country_name)&(mu['UnitDol']=='LCU'),'Mag'] = target_mag\n",
    "    mu.loc[(mu['Member']==country_name)&(mu['UnitDol']=='USD'),'Mag'] = target_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate new column fname\n",
    "au['fname'] = pd.Series(np.arange(au.shape[0])).astype(str).values+'-'+au['Member'].values+'-A-'+au['Classification'].str.replace(' ','').values+au['Subclassification'].str.replace(' ','').values\n",
    "qu['fname'] = pd.Series(np.arange(qu.shape[0])).astype(str).values+'-'+qu['Member'].values+'-Q-'+qu['Classification'].str.replace(' ','').values+qu['Subclassification'].str.replace(' ','').values\n",
    "mu['fname'] = pd.Series(np.arange(mu.shape[0])).astype(str).values+'-'+mu['Member'].values+'-M-'+mu['Classification'].str.replace(' ','').values+mu['Subclassification'].str.replace(' ','').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalize feature values and we store the mean and standard deviation\n",
    "qu_stat = {}\n",
    "for fname in qu['fname'].values:\n",
    "    if not 'RealGDPGrowth' in fname:\n",
    "        data = qu.loc[(qu['fname']==fname),'2000-Q1':'2020-Q4'].values\n",
    "        mean, std = np.nanmean(data), np.nanstd(data)\n",
    "        qu.loc[(qu['fname']==fname),'2000-Q1':'2020-Q4'] = (qu.loc[(qu['fname']==fname),'2000-Q1':'2020-Q4'] - mean) / std\n",
    "        qu_stat[fname] = mean, std\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def deseasonalize(qu = None):\n",
    "    \"\"\"\n",
    "    Deseasonalize quarterly data. We will decompose it first and get trend, seasonality, and random error.\n",
    "    We then remove seasonality and save it to a dictionary to be used later\n",
    "    \n",
    "    Args:\n",
    "        qu - quarterly data\n",
    "    return:\n",
    "        deseasonalized qu\n",
    "    \"\"\"\n",
    "    seasonalities = {}\n",
    "    na_data = qu.loc[(qu['fname'].str.contains('Nationalaccounts')), '2000-Q1':'2019-Q4']\n",
    "    column_ids = qu.loc[(qu['fname'].str.contains('Nationalaccounts')), 'fname'].values\n",
    "    na_data.index = qu.loc[(qu['fname'].str.contains('Nationalaccounts')), 'fname']\n",
    "    na_data = na_data.T\n",
    "    na_data.index = pd.to_datetime(na_data.index)\n",
    "    na_data = na_data.resample('Q').fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    for column_id in column_ids:\n",
    "        seasonalities[column_id] = ''\n",
    "        if not na_data[column_id].isnull().any():\n",
    "            dec_na_data = sm.tsa.seasonal_decompose(na_data[column_id])\n",
    "            qu.loc[(qu['fname']==column_id), '2000-Q1':'2019-Q4'] = na_data.loc[:,column_id].values - dec_na_data.seasonal.values\n",
    "            seasonalities[column_id] = dec_na_data.seasonal\n",
    "    \n",
    "    dta = sm.datasets.co2.load_pandas().data.resample(\"M\").fillna(method='ffill').fillna(method='bfill')\n",
    "    dec_dta = sm.tsa.seasonal_decompose(dta)\n",
    "\n",
    "    return qu, seasonalities\n",
    "\n",
    "### Deseasonalize all National Accounts data\n",
    "qu_sa, seasonalities = deseasonalize(qu = qu.copy())\n",
    "\n",
    "### Draw a sample\n",
    "length = len(qu.loc[qu['fname']=='127-Indonesia-Q-NationalaccountsRealGDP', '2000-Q1':'2019-Q4'].values[0])\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(np.arange(length),qu.loc[qu['fname']=='127-Indonesia-Q-NationalaccountsRealGDP', '2000-Q1':'2019-Q4'].values[0], color='blue')\n",
    "plt.plot(np.arange(length),qu_sa.loc[qu_sa['fname']=='127-Indonesia-Q-NationalaccountsRealGDP', '2000-Q1':'2019-Q4'].values[0], color='red')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Generate data\n",
    "### Target: Quarterly RGDP Growth\n",
    "### Input: ALL non-National Accounts in the same period AND historical National Accounts\n",
    "def gen_q_rgdp_nonNApluslagNA(country_name='Indonesia', qu=None, mu=None):\n",
    "    \"\"\"\n",
    "    Generate a target feature (y) and independent features (x)\n",
    "    \n",
    "    Args:\n",
    "        country_name - country we want to generate the target and features\n",
    "        qu - quarterly dataset\n",
    "        mu - monthly dataset\n",
    "    return:\n",
    "        qu - specific qu for the country\n",
    "        target_fname - target name\n",
    "    \"\"\"\n",
    "    ### Change RGDP Growth of country_name to target\n",
    "    target_fname = qu.loc[(qu['fname'].str.contains(country_name))&(qu['fname'].str.contains('RealGDPGrowth')),'fname'].values[0]\n",
    "    qu.loc[(qu['fname'].str.contains(country_name))&(qu['fname'].str.contains('RealGDPGrowth')),['Classification','fname']]=['target','target']\n",
    "    ### Drop data from 2020-Q1 to 2020-Q4 because mostly it's NA\n",
    "    qu.drop(qu.loc[:,'2020-Q1':'2020-Q4'],axis=1,inplace=True) ## Dropped where there is not enough data\n",
    "    ### Slide National Accounts into last quarter\n",
    "    qu.loc[(qu['fname'].str.contains(country_name))&(qu['fname'].str.contains('Nationalaccounts')),'2000-Q2':'2019-Q4'] = qu.loc[(qu['fname'].str.contains(country_name))&(qu['fname'].str.contains('Nationalaccounts')),'2000-Q1':'2019-Q3'].values\n",
    "    qu.drop(columns = ['2000-Q1'],inplace=True)\n",
    "    ### Drop unnecessary columns\n",
    "    qu.drop(columns=['Member','Indicator','Unit','Classification','Subclassification','Remarks','Mag','UnitDol'],inplace=True)\n",
    "    qu = qu.T\n",
    "    qu.columns = qu.loc['fname',:]\n",
    "    qu.drop('fname',axis=0,inplace=True)\n",
    "    return qu, target_fname\n",
    "\n",
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "    \"\"\"\n",
    "    Generate moving window data\n",
    "    \n",
    "    Args:\n",
    "        dataset - feature array\n",
    "        target - target array\n",
    "        start_index - index where window starts\n",
    "        end_index - index where window ends\n",
    "        history_size - window size\n",
    "        target_size - how many target features we have\n",
    "        step - step of the moving window\n",
    "        single_step - if it's single_step prediction, we only add 1 target feature.\n",
    "    return:\n",
    "        data and labels\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        if single_step:\n",
    "            labels.append(target[i+target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "    \n",
    "    return np.array(data), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data : Indonesia\n",
    "country_name = 'Indonesia'\n",
    "qu_data, target_fname = gen_q_rgdp_nonNApluslagNA(country_name = country_name, qu = qu_sa.copy(), mu = mu.copy())\n",
    "ts = np.array(pd.to_datetime(qu_data.index))\n",
    "\n",
    "### Remove any column that contains NA.\n",
    "### We want to preserve the most data points for target feature\n",
    "for column in qu_data.columns:\n",
    "    countnull = qu_data[column].isnull().sum()\n",
    "    if countnull>0 and column != 'target':\n",
    "        qu_data.drop(column, axis = 1, inplace = True)\n",
    "        \n",
    "y = qu_data['target'].values.astype('float32')\n",
    "X = qu_data.loc[:,qu_data.columns!='target'].values.astype('float32')\n",
    "no_features = X.shape[1]\n",
    "no_target = 1\n",
    "\n",
    "PERCENTAGE_SPLIT = 0.9\n",
    "SPLIT_IDX = int(PERCENTAGE_SPLIT*len(y))\n",
    "\n",
    "### This project wants us to predict the future value.\n",
    "### We will split so that the latest data will be used as testing data.\n",
    "X_train = X[:SPLIT_IDX]\n",
    "X_test = X[SPLIT_IDX:]\n",
    "y_train = y[:SPLIT_IDX]\n",
    "y_test = y[SPLIT_IDX:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model 1 - ID\n",
    "### Hyperparameters\n",
    "EPOCHS = 20000\n",
    "lr = 0.0001\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "epsilon = 1e-07\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(512,activation=tf.keras.activations.linear, input_shape=(no_features,)),\n",
    "  tf.keras.layers.Dense(256,activation=tf.keras.activations.linear),\n",
    "  tf.keras.layers.Dense(128,activation=tf.keras.activations.linear),\n",
    "  tf.keras.layers.Dense(no_features, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(no_target, activation=tf.keras.activations.linear)\n",
    "])\n",
    "opt=tf.keras.optimizers.Adam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, amsgrad=False)\n",
    "model.compile(optimizer=opt, loss='mse', metrics=['mae']) # loss: 0.1800\n",
    "history = model.fit(X_train, y_train, epochs = EPOCHS, batch_size = 1, verbose = 2)\n",
    "\n",
    "### Calculate\n",
    "predictions = model.predict(X_train)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(np.arange(len(y_train)), y_train, color='blue')\n",
    "plt.plot(np.arange(len(y_train)), predictions.flatten(), color='red')\n",
    "\n",
    "### Calculate RMSE for training and testing datasets\n",
    "rmse=np.sqrt(np.mean((y_train-predictions.flatten())**2))\n",
    "print('RMSE Training:', rmse)\n",
    "rmse=np.sqrt(np.mean((y_test-model.predict(X_test).flatten())**2))\n",
    "print('RMSE Testing: ', rmse)\n",
    "\n",
    "### Draw with full data points\n",
    "predictions = model.predict(X)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(np.arange(len(y)),y,color='blue')\n",
    "plt.plot(np.arange(len(y)),predictions.flatten(),color='red')\n",
    "\n",
    "### Will be used later to compare all testing models\n",
    "predictions_fcnn = model.predict(X)\n",
    "predictions_fcnn_test = model.predict(X_test)\n",
    "\n",
    "### Draw error graph\n",
    "plt.figure(figsize=(20,15))\n",
    "for idx, key in enumerate(list(history.history.keys())):\n",
    "    plt.plot(history.history[key][500:EPOCHS], label = key)\n",
    "plt.legend(loc='best')\n",
    "\n",
    "### Save the model\n",
    "model.save_weights('./models/'+str(int(time.time()))+'-Q-nonNAplusNAlast-Dense1-'+country_name+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model 2 - ID\n",
    "model_xgb = xgb.XGBRegressor()\n",
    "history = model_xgb.fit(X_train, y_train)\n",
    "model_xgb.score(X_train,y_train)\n",
    "pickle.dump(model_xgb,open('./models/'+str(int(time.time()))+'-Q-nonNAplusNAlast-XGB-'+country_name+'.h5','wb'))\n",
    "\n",
    "predictions = model_xgb.predict(X_train)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(np.arange(len(y_train)), y_train, color='blue')\n",
    "plt.plot(np.arange(len(y_train)), predictions, color='red')\n",
    "\n",
    "rmse = np.sqrt(np.mean((y_train-predictions.flatten())**2))\n",
    "print('RMSE Training:', rmse)\n",
    "rmse = np.sqrt(np.mean((y_test-model_xgb.predict(X_test))**2))\n",
    "print('RMSE Testing: ', rmse)\n",
    "\n",
    "predictions_xgb = model_xgb.predict(X)\n",
    "predictions_xgb_test = model_xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model 4: Recurrent Neural Networks - Data Processing\n",
    "### Parameters for multivariate_data\n",
    "past_history = 4 ### Window size is 4\n",
    "future_target = 0 ### Predict current target\n",
    "STEP = 1 ### Moving step\n",
    "### Hyperparameters\n",
    "EPOCHS = 10000\n",
    "lr = 0.00005\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "epsilon = 1e-07\n",
    "\n",
    "\n",
    "features = qu_data[qu_data.columns[qu_data.columns!='target']]\n",
    "target = qu_data['target'].values.astype(np.float32)\n",
    "\n",
    "TRAIN_SPLIT = int(features.shape[0]*PERCENTAGE_SPLIT)\n",
    "dataset = features.values.astype(np.float32)\n",
    "\n",
    "x_train_single, y_train_single = multivariate_data(dataset, target, 0,\n",
    "                                                   TRAIN_SPLIT, past_history,\n",
    "                                                   future_target, STEP,\n",
    "                                                   single_step=True)\n",
    "x_val_single, y_val_single = multivariate_data(dataset, target,\n",
    "                                               TRAIN_SPLIT, None, past_history,\n",
    "                                               future_target, STEP,\n",
    "                                               single_step=True)\n",
    "x_raw_single, y_raw_single = multivariate_data(dataset, target,\n",
    "                                               0, None, past_history,\n",
    "                                               future_target, STEP,\n",
    "                                               single_step=True)\n",
    "\n",
    "print ('Single window of past history : {}'.format(x_train_single[0].shape))\n",
    "print(x_train_single.shape)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "BUFFER_SIZE = 1\n",
    "no_features=x_train_single.shape[2]\n",
    "train_data_single = tf.data.Dataset.from_tensor_slices((np.asarray(x_train_single).astype(np.float32), np.asarray(y_train_single).astype(np.float32)))\n",
    "train_data_single = train_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_single = tf.data.Dataset.from_tensor_slices((x_val_single, y_val_single))\n",
    "val_data_single = val_data_single.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "### Model 4 : Input as Time series model\n",
    "single_step_model = tf.keras.models.Sequential()\n",
    "single_step_model.add(tf.keras.layers.Flatten(input_shape = x_train_single[0].shape))\n",
    "single_step_model.add(tf.keras.layers.Dense(no_features*2,activation=tf.keras.activations.linear))\n",
    "single_step_model.add(tf.keras.layers.Dense(no_features,activation=tf.keras.activations.linear))\n",
    "single_step_model.add(tf.keras.layers.Dense(128,activation=tf.keras.activations.linear))\n",
    "single_step_model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))\n",
    "single_step_model.add(tf.keras.layers.Dense(no_target,activation=tf.keras.activations.linear))\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, amsgrad=False)\n",
    "single_step_model.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "EVALUATION_INTERVAL = int(np.ceil(x_train_single.shape[0]/BATCH_SIZE))\n",
    "validation_steps = x_val_single.shape[0] / BATCH_SIZE\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '-RGDP-2DFCNN-' + country_name)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "single_step_history = single_step_model.fit(train_data_single,\n",
    "                                            epochs=EPOCHS,\n",
    "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                            validation_data=val_data_single,\n",
    "                                            validation_steps=validation_steps,\n",
    "                                            callbacks=[tensorboard_callback])\n",
    "\n",
    "predictions = single_step_model.predict(x_train_single)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(np.arange(len(y_train_single)),y_train_single,color='blue')\n",
    "plt.plot(np.arange(len(y_train_single)),predictions.flatten(),color='red')\n",
    "\n",
    "rmse=np.sqrt(np.mean((y_train_single-predictions.flatten())**2))\n",
    "print('RMSE Training:', rmse)\n",
    "rmse=np.sqrt(np.mean((y_val_single-single_step_model.predict(x_val_single).flatten())**2))\n",
    "print('RMSE Testing:', rmse)\n",
    "\n",
    "predictions = single_step_model.predict(x_raw_single)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(np.arange(len(y_raw_single)),y_raw_single,color='blue')\n",
    "plt.plot(np.arange(len(y_raw_single)),predictions.flatten(),color='red')\n",
    "\n",
    "\n",
    "\n",
    "#### Plotting\n",
    "loss=single_step_history.history['loss']\n",
    "val_loss=single_step_history.history['val_loss']\n",
    "epochs=range(len(loss)) # Get number of epochs\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "single_step_model.save_weights('./models/'+str(int(time.time()))+'-Q-nonNAplusNAlast-2DFCNN-'+country_name+'.h5')\n",
    "\n",
    "predictions_2dfcnn = single_step_model.predict(x_raw_single)\n",
    "predictions_2dfcnn_test = single_step_model.predict(x_raw_single[-len(y_test):,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Draw the comparison between Actual and Model Predictions\n",
    "### Training\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(ts, y, label = 'Actual', color = 'blue')\n",
    "plt.plot(ts, predictions_fcnn.flatten(), label = 'FCNN', color = 'red')\n",
    "plt.plot(ts, predictions_xgb.flatten(), label = 'XGB', color = 'green')\n",
    "plt.plot(ts[4:len(y)], predictions_2dfcnn.flatten() * std + mean, label = '2D FCNN', color = 'orange')\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "### Testing\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(ts[-len(y_test):], y_test, label = 'Actual', color = 'blue')\n",
    "plt.plot(ts[-len(y_test):], predictions_fcnn_test.flatten(), label = 'FCNN', color = 'red')\n",
    "plt.plot(ts[-len(y_test):], predictions_xgb_test.flatten(), label = 'XGB', color = 'green')\n",
    "plt.plot(ts[-len(predictions_2dfcnn_test.flatten()):], predictions_2dfcnn_test.flatten(), label = '2D FCNN', color = 'orange')\n",
    "plt.legend(loc = 'best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data : China\n",
    "country_name = 'Republic of China'\n",
    "qu_data, target_fname = gen_q_rgdp_nonNApluslagNA(country_name = country_name, qu = qu_sa.copy(), mu = mu.copy())\n",
    "ts = np.array(pd.to_datetime(qu_data.index))\n",
    "\n",
    "### Remove any column that contains NA.\n",
    "### We want to preserve the most data points for target feature\n",
    "for column in qu_data.columns:\n",
    "    countnull = qu_data[column].isnull().sum()\n",
    "    if countnull>0 and column != 'target':\n",
    "        qu_data.drop(column, axis = 1, inplace = True)\n",
    "        \n",
    "y = qu_data['target'].values.astype('float32')\n",
    "X = qu_data.loc[:,qu_data.columns!='target'].values.astype('float32')\n",
    "no_features = X.shape[1]\n",
    "no_target = 1\n",
    "\n",
    "### Training 90%, testing 10%\n",
    "PERCENTAGE_SPLIT = 0.9\n",
    "SPLIT_IDX = int(PERCENTAGE_SPLIT*len(y))\n",
    "\n",
    "### This project wants us to predict the future value.\n",
    "### We will split so that the latest data will be used as testing data.\n",
    "X_train = X[:SPLIT_IDX]\n",
    "X_test = X[SPLIT_IDX:]\n",
    "y_train = y[:SPLIT_IDX]\n",
    "y_test = y[SPLIT_IDX:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model 1 - China\n",
    "### Hyperparameters\n",
    "EPOCHS = 20000\n",
    "lr = 0.0001\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "epsilon = 1e-07\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(512,activation=tf.keras.activations.linear, input_shape=(no_features,)),\n",
    "  tf.keras.layers.Dense(256,activation=tf.keras.activations.linear),\n",
    "  tf.keras.layers.Dense(128,activation=tf.keras.activations.linear),\n",
    "  tf.keras.layers.Dense(no_features, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(no_target, activation=tf.keras.activations.linear)\n",
    "])\n",
    "opt=tf.keras.optimizers.Adam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, amsgrad=False)\n",
    "model.compile(optimizer=opt, loss='mse', metrics=['mae']) # loss: 0.1800\n",
    "history = model.fit(X_train, y_train, epochs = EPOCHS, batch_size = 1, verbose = 2)\n",
    "\n",
    "### Calculate\n",
    "predictions = model.predict(X_train)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(np.arange(len(y_train)), y_train, color='blue')\n",
    "plt.plot(np.arange(len(y_train)), predictions.flatten(), color='red')\n",
    "\n",
    "### Calculate RMSE for training and testing datasets\n",
    "rmse=np.sqrt(np.mean((y_train-predictions.flatten())**2))\n",
    "print('RMSE Training:', rmse)\n",
    "rmse=np.sqrt(np.mean((y_test-model.predict(X_test).flatten())**2))\n",
    "print('RMSE Testing: ', rmse)\n",
    "\n",
    "### Draw with full data points\n",
    "predictions = model.predict(X)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(np.arange(len(y)),y,color='blue')\n",
    "plt.plot(np.arange(len(y)),predictions.flatten(),color='red')\n",
    "\n",
    "### Will be used later to compare all testing models\n",
    "predictions_fcnn = model.predict(X)\n",
    "predictions_fcnn_test = model.predict(X_test)\n",
    "\n",
    "### Draw error graph\n",
    "plt.figure(figsize=(20,15))\n",
    "for idx, key in enumerate(list(history.history.keys())):\n",
    "    plt.plot(history.history[key][500:EPOCHS], label = key)\n",
    "plt.legend(loc='best')\n",
    "\n",
    "### Save the model\n",
    "model.save_weights('./models/'+str(int(time.time()))+'-Q-nonNAplusNAlast-Dense1-'+country_name+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model 2 - China\n",
    "model_xgb = xgb.XGBRegressor()\n",
    "history = model_xgb.fit(X_train, y_train)\n",
    "model_xgb.score(X_train,y_train)\n",
    "pickle.dump(model_xgb,open('./models/'+str(int(time.time()))+'-Q-nonNAplusNAlast-XGB-'+country_name+'.h5','wb'))\n",
    "\n",
    "predictions = model_xgb.predict(X_train)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(np.arange(len(y_train)), y_train, color='blue')\n",
    "plt.plot(np.arange(len(y_train)), predictions, color='red')\n",
    "\n",
    "rmse = np.sqrt(np.mean((y_train-predictions.flatten())**2))\n",
    "print('RMSE Training:', rmse)\n",
    "rmse = np.sqrt(np.mean((y_test-model_xgb.predict(X_test))**2))\n",
    "print('RMSE Testing: ', rmse)\n",
    "\n",
    "predictions_xgb = model_xgb.predict(X)\n",
    "predictions_xgb_test = model_xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model 4: Recurrent Neural Networks - Data Processing\n",
    "### Parameters for multivariate_data\n",
    "past_history = 4 ### Window size is 4\n",
    "future_target = 0 ### Predict current target\n",
    "STEP = 1 ### Moving step\n",
    "### Hyperparameters\n",
    "EPOCHS = 10000\n",
    "lr = 0.00005\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "epsilon = 1e-07\n",
    "\n",
    "\n",
    "features = qu_data[qu_data.columns[qu_data.columns!='target']]\n",
    "target = qu_data['target'].values.astype(np.float32)\n",
    "\n",
    "TRAIN_SPLIT = int(features.shape[0]*PERCENTAGE_SPLIT)\n",
    "dataset = features.values.astype(np.float32)\n",
    "\n",
    "x_train_single, y_train_single = multivariate_data(dataset, target, 0,\n",
    "                                                   TRAIN_SPLIT, past_history,\n",
    "                                                   future_target, STEP,\n",
    "                                                   single_step=True)\n",
    "x_val_single, y_val_single = multivariate_data(dataset, target,\n",
    "                                               TRAIN_SPLIT, None, past_history,\n",
    "                                               future_target, STEP,\n",
    "                                               single_step=True)\n",
    "x_raw_single, y_raw_single = multivariate_data(dataset, target,\n",
    "                                               0, None, past_history,\n",
    "                                               future_target, STEP,\n",
    "                                               single_step=True)\n",
    "\n",
    "print ('Single window of past history : {}'.format(x_train_single[0].shape))\n",
    "print(x_train_single.shape)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "BUFFER_SIZE = 1\n",
    "no_features=x_train_single.shape[2]\n",
    "train_data_single = tf.data.Dataset.from_tensor_slices((np.asarray(x_train_single).astype(np.float32), np.asarray(y_train_single).astype(np.float32)))\n",
    "train_data_single = train_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_single = tf.data.Dataset.from_tensor_slices((x_val_single, y_val_single))\n",
    "val_data_single = val_data_single.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "### Model 4 : Input as Time series model\n",
    "single_step_model = tf.keras.models.Sequential()\n",
    "single_step_model.add(tf.keras.layers.Flatten(input_shape = x_train_single[0].shape))\n",
    "single_step_model.add(tf.keras.layers.Dense(no_features*2,activation=tf.keras.activations.linear))\n",
    "single_step_model.add(tf.keras.layers.Dense(no_features,activation=tf.keras.activations.linear))\n",
    "single_step_model.add(tf.keras.layers.Dense(128,activation=tf.keras.activations.linear))\n",
    "single_step_model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))\n",
    "single_step_model.add(tf.keras.layers.Dense(no_target,activation=tf.keras.activations.linear))\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, amsgrad=False)\n",
    "single_step_model.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "EVALUATION_INTERVAL = int(np.ceil(x_train_single.shape[0]/BATCH_SIZE))\n",
    "validation_steps = x_val_single.shape[0] / BATCH_SIZE\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '-RGDP-2DFCNN-' + country_name)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "single_step_history = single_step_model.fit(train_data_single,\n",
    "                                            epochs=EPOCHS,\n",
    "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                            validation_data=val_data_single,\n",
    "                                            validation_steps=validation_steps,\n",
    "                                            callbacks=[tensorboard_callback])\n",
    "\n",
    "predictions = single_step_model.predict(x_train_single)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(np.arange(len(y_train_single)),y_train_single,color='blue')\n",
    "plt.plot(np.arange(len(y_train_single)),predictions.flatten(),color='red')\n",
    "\n",
    "rmse=np.sqrt(np.mean((y_train_single-predictions.flatten())**2))\n",
    "print('RMSE Training:', rmse)\n",
    "rmse=np.sqrt(np.mean((y_val_single-single_step_model.predict(x_val_single).flatten())**2))\n",
    "print('RMSE Testing:', rmse)\n",
    "\n",
    "predictions = single_step_model.predict(x_raw_single)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(np.arange(len(y_raw_single)),y_raw_single,color='blue')\n",
    "plt.plot(np.arange(len(y_raw_single)),predictions.flatten(),color='red')\n",
    "\n",
    "\n",
    "\n",
    "#### Plotting\n",
    "loss=single_step_history.history['loss']\n",
    "val_loss=single_step_history.history['val_loss']\n",
    "epochs=range(len(loss)) # Get number of epochs\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "single_step_model.save_weights('./models/'+str(int(time.time()))+'-Q-nonNAplusNAlast-2DFCNN-'+country_name+'.h5')\n",
    "\n",
    "predictions_2dfcnn = single_step_model.predict(x_raw_single)\n",
    "predictions_2dfcnn_test = single_step_model.predict(x_raw_single[-len(y_test):,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Draw the comparison between Actual and Model Predictions\n",
    "### Training\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(ts, y, label = 'Actual', color = 'blue')\n",
    "plt.plot(ts, predictions_fcnn.flatten(), label = 'FCNN', color = 'red')\n",
    "plt.plot(ts, predictions_xgb.flatten(), label = 'XGB', color = 'green')\n",
    "plt.plot(ts[4:len(y)], predictions_2dfcnn.flatten() * std + mean, label = '2D FCNN', color = 'orange')\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "### Testing\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(ts[-len(y_test):], y_test, label = 'Actual', color = 'blue')\n",
    "plt.plot(ts[-len(y_test):], predictions_fcnn_test.flatten(), label = 'FCNN', color = 'red')\n",
    "plt.plot(ts[-len(y_test):], predictions_xgb_test.flatten(), label = 'XGB', color = 'green')\n",
    "plt.plot(ts[-len(predictions_2dfcnn_test.flatten()):], predictions_2dfcnn_test.flatten(), label = '2D FCNN', color = 'orange')\n",
    "plt.legend(loc = 'best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
